import os

os.environ['OMP_NUM_THREADS'] = '1'
import argparse
import torch
from src.env import MultipleEnvironments
from src.model import PPO
from src.process import eval
import torch.multiprocessing as _mp
from torch.distributions import Categorical
import torch.nn.functional as F
import numpy as np
import shutil
from gym_super_mario_bros.actions import RIGHT_ONLY
from collections import deque


def get_args():
    parser = argparse.ArgumentParser()
    # å¼ºåˆ¶åªå‘å³åŠ¨ä½œç©ºé—´ï¼Œä¿ç•™å‚æ•°ä½†ä¸ä½¿ç”¨ç”¨æˆ·ä¼ å…¥å€¼
    parser.add_argument("--action_type", type=str, default="right", choices=["right", "simple", "complex"])
    parser.add_argument('--lr', type=float, default=1e-4)
    parser.add_argument('--gamma', type=float, default=0.9)
    parser.add_argument('--tau', type=float, default=1.0)
    parser.add_argument('--beta', type=float, default=0.01)
    parser.add_argument('--epsilon', type=float, default=0.2)
    parser.add_argument('--batch_size', type=int, default=16)
    parser.add_argument('--num_epochs', type=int, default=10)
    parser.add_argument("--num_local_steps", type=int, default=512)
    parser.add_argument("--num_global_steps", type=int, default=int(5e6))
    parser.add_argument("--num_processes", type=int, default=8)
    parser.add_argument("--save_interval", type=int, default=50)  # ä¿ç•™ä½†ä¸æŒ‰é—´éš”ä¿å­˜
    parser.add_argument("--max_actions", type=int, default=200)
    parser.add_argument("--log_path", type=str, default="tensorboard/ppo_super_mario_bros")
    parser.add_argument("--saved_path", type=str, default="trained_models")
    parser.add_argument("--output_path", type=str, default=None)

    # Starting level
    parser.add_argument("--world", type=int, default=1)
    parser.add_argument("--stage", type=int, default=1)

    args = parser.parse_args()
    return args


# ===== åŠ¨æ€å­¦ä¹ ç‡è®¾ç½®å‡½æ•°ï¼ˆä»…æ·»åŠ æ­¤åŠŸèƒ½ï¼Œå…¶ä½™ä¿æŒä¸å˜ï¼‰=====
def get_dynamic_lr(world, stage):
    # 1-1 åˆ° 2-4
    if world <= 2:
        return 1e-3
    # 3-1 åˆ° 5-4
    elif 3 <= world <= 5:
        return 1e-4
    # 6-1 ä¹‹åæ›´ç²¾ç¡®ï¼ˆæ›´å°çš„å­¦ä¹ ç‡ï¼‰
    else:
        return 5e-5


def train(opt):
    if torch.cuda.is_available():
        torch.cuda.manual_seed(123)
    else:
        torch.manual_seed(123)

    if os.path.isdir(opt.log_path):
        shutil.rmtree(opt.log_path)
    os.makedirs(opt.log_path, exist_ok=True)
    os.makedirs(opt.saved_path, exist_ok=True)

    mp = _mp.get_context("spawn")

    # å¼ºåˆ¶ä½¿ç”¨ RIGHT_ONLY
    opt.action_type = "right"

    # Initialize starting level
    curr_world = opt.world
    curr_stage = opt.stage

    print(f"ğŸš€ Starting training on World {curr_world}-{curr_stage}")
    envs = MultipleEnvironments(opt.action_type, opt.num_processes, curr_world, curr_stage, opt.output_path)

    # åªä½¿ç”¨ RIGHT_ONLY çš„åŠ¨ä½œæ•°é‡
    num_actions = len(RIGHT_ONLY)
    num_states = 4

    model = PPO(num_states, num_actions)
    if torch.cuda.is_available():
        model.cuda()
    model.share_memory()

    # Start evaluation process
    process = mp.Process(target=eval, args=(opt, model, num_states, num_actions))
    process.start()

    # ===== ä½¿ç”¨åŠ¨æ€å­¦ä¹ ç‡åˆå§‹åŒ–ä¼˜åŒ–å™¨ =====
    curr_lr = get_dynamic_lr(curr_world, curr_stage)
    optimizer = torch.optim.Adam(model.parameters(), lr=curr_lr)
    print(f"âš™ï¸ åˆå§‹å­¦ä¹ ç‡è®¾ç½®ä¸º {curr_lr}")

    # Initialize environments
    [agent_conn.send(("reset", None)) for agent_conn in envs.agent_conns]
    curr_states_data = [agent_conn.recv() for agent_conn in envs.agent_conns]
    curr_states = torch.from_numpy(np.concatenate(curr_states_data, 0))
    if torch.cuda.is_available():
        curr_states = curr_states.cuda()

    # æœ€è¿‘ 5 ä¸ª episode çš„é€šå…³è®°å½•ï¼ˆTrue/Falseï¼‰
    recent_passes = deque(maxlen=5)

    curr_episode = 0
    while True:
        curr_episode += 1
        old_log_policies = []
        actions_list = []
        values = []
        states = []
        rewards = []
        dones = []

        # Track if level is cleared in this batch
        level_cleared_in_batch = False

        for _ in range(opt.num_local_steps):
            states.append(curr_states)
            logits, value = model(curr_states)
            values.append(value.squeeze())
            policy = F.softmax(logits, dim=1)
            old_m = Categorical(policy)
            action = old_m.sample()
            actions_list.append(action)
            old_log_policy = old_m.log_prob(action)
            old_log_policies.append(old_log_policy)

            if torch.cuda.is_available():
                [agent_conn.send(("step", act)) for agent_conn, act in zip(envs.agent_conns, action.cpu())]
            else:
                [agent_conn.send(("step", act)) for agent_conn, act in zip(envs.agent_conns, action)]

            step_results = [agent_conn.recv() for agent_conn in envs.agent_conns]
            state_list = [r[0] for r in step_results]
            reward_list = [r[1] for r in step_results]
            done_list = [r[2] for r in step_results]
            info_list = [r[3] for r in step_results]

            # Check for flag_get (Level Complete)
            for info in info_list:
                if info.get("flag_get", False):
                    level_cleared_in_batch = True

            state = torch.from_numpy(np.concatenate(state_list, 0))

            reward = torch.from_numpy(np.array(reward_list, dtype=np.float32))
            done = torch.from_numpy(np.array(done_list, dtype=np.float32))

            if torch.cuda.is_available():
                state = state.cuda()
                reward = reward.cuda()
                done = done.cuda()
            else:
                reward = torch.FloatTensor(reward_list)
                done = torch.FloatTensor(done_list)

            rewards.append(reward)
            dones.append(done)
            curr_states = state

        avg_reward = torch.stack(rewards).mean().item()

        _, next_value = model(curr_states)
        next_value = next_value.squeeze()

        old_log_policies = torch.cat(old_log_policies).detach()
        actions = torch.cat(actions_list)
        values = torch.cat(values).detach()
        states = torch.cat(states)

        gae = 0
        R = []
        for value, reward, done in list(zip(values, rewards, dones))[::-1]:
            gae = gae * opt.gamma * opt.tau
            gae = gae + reward + opt.gamma * next_value.detach() * (1 - done) - value.detach()
            next_value = value
            R.append(gae + value)
        R = R[::-1]
        R = torch.cat(R).detach()
        advantages = R - values

        for i in range(opt.num_epochs):
            indice = torch.randperm(opt.num_local_steps * opt.num_processes)
            for j in range(opt.batch_size):
                batch_indices = indice[
                    int(j * (opt.num_local_steps * opt.num_processes / opt.batch_size)):
                    int((j + 1) * (opt.num_local_steps * opt.num_processes / opt.batch_size))
                ]
                logits, value = model(states[batch_indices])
                new_policy = F.softmax(logits, dim=1)
                new_m = Categorical(new_policy)
                new_log_policy = new_m.log_prob(actions[batch_indices])
                ratio = torch.exp(new_log_policy - old_log_policies[batch_indices])
                actor_loss = -torch.mean(
                    torch.min(
                        ratio * advantages[batch_indices],
                        torch.clamp(ratio, 1.0 - opt.epsilon, 1.0 + opt.epsilon) * advantages[batch_indices],
                    )
                )
                critic_loss = F.smooth_l1_loss(R[batch_indices], value.squeeze())
                entropy_loss = torch.mean(new_m.entropy())
                total_loss = actor_loss + critic_loss - opt.beta * entropy_loss
                optimizer.zero_grad()
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)
                optimizer.step()

        print(
            f"Episode: {curr_episode}. World {curr_world}-{curr_stage}. Loss: {total_loss:.4f}. Avg Reward: {avg_reward:.2f}"
        )

        # è®°å½•æœ¬ episode æ˜¯å¦é€šå…³ï¼Œå¹¶è®¡ç®—æœ€è¿‘ 5 ä¸ªçš„é€šè¿‡ç‡
        recent_passes.append(level_cleared_in_batch)
        pass_rate = sum(recent_passes) / len(recent_passes)
        print(f"ğŸ“ˆ Recent pass rate (last {len(recent_passes)}): {pass_rate:.2f}")

        # ===== æ¡ä»¶ä¿å­˜é€šç”¨æ¨¡å‹ï¼ˆä»…å½“æœ€è¿‘ 5 ä¸ª episode é€šè¿‡ç‡ >= 0.7ï¼‰=====
        if len(recent_passes) == recent_passes.maxlen and pass_rate >= 0.7:
            save_path = os.path.join(opt.saved_path, "ppo_super_mario_bros_continuous")
            torch.save(model.state_dict(), save_path)
            print(f"âœ… Pass rate >= 70%. General model saved to {save_path}")

        # ===== Automatic Curriculum Switching =====
        if level_cleared_in_batch:
            print(f"ğŸ‰ Level {curr_world}-{curr_stage} CLEARED! Switching level...")

            # ä»…å½“æœ€è¿‘ 5 ä¸ª episode é€šè¿‡ç‡ >= 70% æ—¶ï¼Œä¿å­˜å…³å¡ä¸“ç”¨æ¨¡å‹
            if len(recent_passes) == recent_passes.maxlen and pass_rate >= 0.7:
                save_path = os.path.join(opt.saved_path, f"ppo_cleared_{curr_world}_{curr_stage}")
                torch.save(model.state_dict(), save_path)
                print(f"ğŸ† Pass rate >= 70%. Checkpoint saved: {save_path}")
            else:
                print("ğŸŸ¡ Pass rate below 70%. Skip saving checkpoint for this level.")

            # 2. Advance to next level
            curr_stage += 1
            if curr_stage > 4:
                curr_stage = 1
                curr_world += 1

            # 3. Close old environments to free memory
            print("ğŸ”„ Closing old environments...")
            envs.close()

            # 4. Create new environments
            print(f"ğŸš€ Switching to World {curr_world}-{curr_stage}")
            envs = MultipleEnvironments(opt.action_type, opt.num_processes, curr_world, curr_stage, opt.output_path)

            # 5. Reset new environments and state
            [agent_conn.send(("reset", None)) for agent_conn in envs.agent_conns]
            curr_states_data = [agent_conn.recv() for agent_conn in envs.agent_conns]
            curr_states = torch.from_numpy(np.concatenate(curr_states_data, 0))
            if torch.cuda.is_available():
                curr_states = curr_states.cuda()

            # 6. åŒæ­¥æ›´æ–°å­¦ä¹ ç‡ï¼ˆæ ¹æ®æ–°å…³å¡ï¼‰
            curr_lr = get_dynamic_lr(curr_world, curr_stage)
            for param_group in optimizer.param_groups:
                param_group['lr'] = curr_lr
            print(f"ğŸ”§ å­¦ä¹ ç‡æ›´æ–°ä¸º {curr_lr} (World {curr_world}-{curr_stage})")

            # Reset episode count for the new level (optional)
            curr_episode = 0
            # æ¸…ç©ºå†å²é€šè¿‡è®°å½•ï¼Œé¿å…è·¨å…³å¡ç»Ÿè®¡æ··æ·†
            recent_passes.clear()


if __name__ == "__main__":
    opt = get_args()
    train(opt)
